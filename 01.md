# Математическая статистика

Есть числа $x_1, \ldots, x_n$. Пусть они характеризуют какой-то процесс. Набор этих действительных чисел называется _выборкой_.

Делается допущение, что каждое из этих чисел является реализацией значения некоторой случайной величины.

Случайно выбирается элементарное событие, к нему применяется функция $\xi_1$ и как результат получаем $x_1$ и т.д.

Мы всегда будем предполагать, что рассматривается выборка из независимых реализаций некоторой случайной величины

$$
\xi_1, \ldots, \xi_n: \Omega \rightarrow \mathbb{R}, \xi_1(w) = x_1, \ldots, \xi_n(w) = x_n
$$

$\xi_i$ - независимы в совокупности и одинаково распределены.

Основная задача статистики состоит в том, чтобы зная $x_1, \ldots, x_n$, как можно точнее и правдоподобнее оценить распределение случайных величин $\xi_1, \ldots, \xi_n$.

## Точечная оценка параметров распределений

Когда говорим про параметры распределений, имеется в виду, что функция распределения $i$-ой случайной величины породившей выборку $F_{\xi_i}$ известна с точностью до каких-то своих параметров, т.е. $F_{\xi_i}\in \{ F_{\overline{\theta}}\}_{\theta \in \Theta}$, где $\Theta\subset\mathbb{R}^m$, принадлежит какому-то классу функций $F_{\overline{\theta}}$, где само $\Theta$ является неизвестным параметром $\theta \in \Theta$, которое само (имеется в виду $\Theta$) является подмножеством в $m$-мерном пространстве, т.е. параметр это не обязательно одно неизвестное число, это может быть несколько чисел.

Например, мы знаем, что наша выборка порождена нормальным распределением, $\xi_i \thicksim N(\theta_1, \theta_2^2)$, тогда  $\theta=(\theta_1, \theta_2^2)$ - пара, а переметрическое множество $\Theta = \mathbb{R} \times \mathbb{R}_{+}$. $\mathbb{R}$ - так как среднее может быть как положительным, так и отрицательным, а $\mathbb{R}_{+}$ - так как дисперсия всегда положительна.

Оценка параметров - это значит, что нам хочется на основе нашей выборки придумать какие-нибудь оценки для этих неизвестных параметров.

Оценка (точечная) - это любая функция от элементов выборки, которая призвана каким-то образом апроксимировать неизвестный параметр, т.е. принимает значение в параметрическом множестве - $\Theta$.

Пусть придумаем некоторую оценку математического ожидания, например, такую $(x_1 - x_2 + x_3 - \sin x_n)^3$. Потом придумаем другую $(x_1/2 + x_2 + \cdots + x_n)$.

Как оценить какая оценка лучше, а какая хуже?

Оценки $\widehat{\theta} = (x_1, \ldots, x_n)$ - функция от элементов выборки. Одна выборка - одни элементы, другая выборка  - другие.

Но наверное нужно считать, что аргументами оценки служат не просто числа, а те случайные величины, которые эти числа породили - $\xi_1, \ldots, \xi_n$

Реально оценка это не просто число, это не значение на конкретной выборке, которую мы наблюдали, оценка - это функция от всех возможных наборов, которые могут получиться в качестве реализации случайной величины $\xi_1, \ldots, \xi_n$ с фунцией распределения $F_{\xi_i}$, принадлежащая этому классу $\{F_{\overline{\theta}}\}$. Реально нас интересует случайная величина $\widehat{\theta}_n(\xi_1, \ldots, \xi_n)$, т.е. функция от случайных величин, которые породили нашу выборку

Оценим две характеристики качества:

1. Несмещенная оценка

Это такая случайная величина, такая функция от элементов выборки, что в среднем она дает в точности параметр $\theta$. Считаем, что оцениваем пока только один параметр ($m=1$). Мы оцениваем по отдельности каждую компоненму этого вектора $\theta$.

$$
E\widehat{\theta}_n = \theta
$$

2. Состоятельная оценка

Если случайная величина $\widehat{\theta}_n$ по вероятности сходится к неизвестному параметру $\theta$ на всем параметрическом множестве для $\forall \theta \in \Theta$.

$$
\widehat{\theta}_n \xrightarrow[n\to\infty]{P} \theta
$$

> Сходимость по вероятности: $\forall \varepsilon > 0\ P(\mid \widehat{\theta}_n - \theta\mid > \varepsilon) \xrightarrow[n\to\infty]{}0$

Смысл состоятельности в том, что мера множества тех элементарных исходов, на которых значения оценки отличается от неизвестного параметра больше чем на очень маленькую, наперед заданную величину. Эта мера с ростом выборки уменьшается. Чем больше размер выборки, тем меньше вероятность, что мы ошибаемся, оценивая с помощью нашей оценки, этот неизвестный параметр больше, чем на заданную наперед $\varepsilon$, каким бы ни была $\varepsilon$. С ростом объема выборки незначительное отклонение имеет вероятность все ближе к $0$.

**Пример**

Хотим оценить среднее значение $\theta_1$ в нормальном распределении $N(\theta_1, \theta_2^2)$.

Рассмотрим среднее арифметическое $\overline{x}$ - выборочное среднее.
$$
\widehat{\theta}_n = \overline{x} = \frac{1}{n} \sum\limits_{i=1}^n \xi_i
$$

Найдем математическое ожидание
$$
E\widehat{\theta}_n = \frac{1}{n} \sum\limits_{i=1}^n E\xi_i = \frac{1}{n} \sum\limits_{i=1}^n \theta_1 = \theta_1
$$

Следовательно наша оценка - несмещенная.

Но если в качестве оценки взять $\widehat{\theta}_n = \displaystyle \frac{2}{n} \sum\limits_{i=1}^n \xi_i$, то $E\widehat{\theta}_n = 2\cdot \theta_1$ и это уже не будет несмещенной оценкой.

Является ли $\overline{x}$ - состоятельной оценкой?

Да, является. Так как $\widehat{\theta}_n = \displaystyle\frac{\xi_1 + \cdots + \xi_n}{n}$.

Имеем по ЗБЧ, что если есть независимые одинаково распределенные случайные величины имеющие конечное математическое ожидание, то их среднее арифметическое сходится по вероятности к математическому ожиданию каждой из них.
$$
\widehat{\theta}_n = \displaystyle\frac{\xi_1 + \cdots + \xi_n}{n}\xrightarrow[n\to\infty]{P} E\xi_i = \theta_1
$$

Т.е. состоятельность оценки следует из ЗБЧ. 

По УЗБЧ, сходимость "по вероятности" можно заменить на сходимость "почти наверное". В этом случае оценку называют _сильно состоятельной_.

**Пример**

Оценка несмещенная, но не состоятельная. $\xi_i \thicksim N(\theta_1, \theta_2^2)$.

В качестве оценки выберем $\widehat{\theta}_n = \xi_1$, т.е. среднее оценивается по первому элементу выборки.

**Пример**

$\xi_i \thicksim N(a, \theta_2^2$, т.е. математическое ожидание известно и равно $a$, т.е. $E\xi_i = a$

$$
\widehat{\theta}_n = \frac{1}{n}\sum\limits_{i=1}^n \Big( \xi_i - a\Big)^2
$$

Получили несмещенную и состоятельную оценку.

**Пример**

$\xi_i \thicksim N(\theta_1, \theta_2^2$, т.е. математическое ожидание не известно. Найдем $\theta_2$.

Выберем в качестве оценки
$$
\widehat{\theta}_n = \frac{1}{n}\sum\limits_{i=1}^n \Big( \xi_i - \frac{1}{n}\sum\limits_{j=1}^n \xi_j\Big)^2
$$

При этом подставим оценку математического ожидания, как среднее - $\overline{x}$.

Найдем математическое ожидание оценки

$$
E\widehat{\theta}_n = \frac{1}{n}\sum\limits_{i=1}^n E \Bigg( \xi_i^2 - \frac{2}{n} \xi_i \sum\limits_{j=1}^n \xi_j + \frac{1}{n^2} \Big(\xi_1^2 + \cdots + \xi_n^2 + \sum\limits_{j_1 \not = j_2} \xi_{j_1} \cdot \xi_{j_2}\Big)\Bigg) =
$$

В сумме $\sum\limits_{j_1 \not = j_2}$ пары $j_1, j_2$ - упорядочены, т.е. имеются слагаемые $\xi_{j_1} \cdot \xi_{j_2}$ и $\xi_{j_2} \cdot \xi_{j_1}$, поэтому не пишем коэффициент $2$.

$$
= \frac{1}{n} \sum\limits_{i=1}^n \Bigg(E\xi_i^2 - \frac{2}{n} E \Big( \xi_i\cdot\xi_1 + \xi_i\cdot\xi_2 + \cdots + \xi_i\cdot\xi_i + \cdots \xi_i\cdot\xi_n \Big) + \frac{n}{n^2}E\xi_i^2 + \frac{n(n-1)}{n^2}(E\xi_i)^2\Bigg)
$$

В сумме $\xi_i\cdot\xi_1 + \xi_i\cdot\xi_2 + \cdots + \xi_i\cdot\xi_i + \cdots \xi_i\cdot\xi_n$ одно слагаемое $\xi_i^2$, а остальные перемножения независимых случайных величин $\Rightarrow E(\xi_i\xi_j) = E(\xi_i)\cdot E(\xi_j)$, а так как они одинаково распределены - $E(\xi_i\xi_j) = (E\xi_i)^2$

$$
= \frac{1}{n} \sum\limits_{i=1}^n \Bigg(E\xi_i^2 - \frac{2}{n} \Big( (n-1)\cdot (E\xi_i)^2 + E\xi_i^2 \Big) + \frac{1}{n}E\xi_i^2 + \frac{(n-1)}{n}(E\xi_i)^2\Bigg) = \\
\ \\
= \frac{1}{n} \sum\limits_{i=1}^n \Bigg( \Big(E\xi_i^2\Big)\Big( 1 - \frac{2}{n} + \frac{1}{n} \Big) + (E\xi_i)^2 \Big( - \frac{2(n-1)}{n} + \frac{n-1}{n} \Big)\Bigg) = \\
\ \\
= \frac{1}{n} \sum\limits_{i=1}^n \Bigg( \Big(E\xi_i^2\Big)\frac{n-1}{n} - (E\xi_i)^2 \frac{n-1}{n} \Bigg) = 
$$

$\frac{1}{n} \sum\limits_{i=1}^n = 1$, т.к. под суммой стоят $E\xi_i$ и $E\xi_i^2$, которые от $i$ не зависит, так как одинаково распределены.

$$
= \frac{n-1}{n} \Bigg(E\xi_i^2 - (E\xi_i)^2 \Bigg) = \frac{n-1}{n} \theta_2^2
$$

Оценка не является несмещенной.

Чтобы получилась несмещенная оценка нужно взять
$$
\widehat{\theta}_n = \frac{1}{n-1}\sum\limits_{i=1}^n \Big( \xi_i - \frac{1}{n}\sum\limits_{j=1}^n \xi_j\Big)^2
$$

Можно было догадаться, что сумму надо делить на $n-1$, а не на $n$.

Заметим, что
$$
\sum\limits_{i=1}^n \Big( \xi_i - \frac{1}{n}\sum\limits_{j=1}^n \xi_j\Big) = 0
$$

Получается, что есть некоторое линейное соотношение. Другими словами есть $n$-мерное пространство, а в нем гиперплоскость, задаваемая этим линейным соотношением. Эта гиперплоскость имеет размерность $n-1$.

Между слагаемыми в сумме основной формулы есть **одна** линейная зависимость.

Существуют случаи, когда для некоторого параметра просто нельзя сделать несмещенную оценку.

**Пример**

Дана выборка $x_1, \ldots, x_n$, которая порождена случайными величинами $\xi_1, \ldots, \xi_n$. Пусть $\xi_i \thicksim Binom(1,p)$, где $p$ - неизвестен.

Это схема Бернулли с $1$-м бросанием монеты и $p$- вероятностью, что выпадет решка.

$\xi_1, \ldots, \xi_n$ - последовательность, состоящая из $0$ и $1$, где $0$ - неудача (выпал орел), $1$ - успех (выпала решка).

> $Binom(k,p)$ - это случайная величина, которая равна числу успехов в $k$ испытаниях схемы Бернулли ($k$ бросаний монетки), с вероятностью успеха $p$, т.е. случайная величина принимает значения от $0$ до $k$.

> Понятно, что если $x_i$ принимает значения $-1, 0, 1$, то это не $Binom$ распределение случайной величины

Считаем, что оценивать будем не $p$, а $\ln p$, т.е. $\theta = \ln p$ - логарифм вероятности успеха.

Пусть существует несмещенная оценка этого параметра - $\widehat{\theta}_n = \widehat{\theta}_n(\xi_1, \ldots, \xi_n)$.

По определению $E\widehat{\theta}_n(\xi_1, \ldots, \xi_n) := \ln p$.

А теперь подсчитаем $E\widehat{\theta}_n(\xi_1, \ldots, \xi_n)$ прямо.

$$
E\widehat{\theta}_n(\xi_1, \ldots, \xi_n) = \sum\limits_{x_1=0}^1 \ldots \sum\limits_{x_n=0}^1 \widehat{\theta}_n(x_1, \ldots, x_n) \cdot P(\xi_1 = x_1, \ldots, \xi_n = x_n) = 
$$

$\widehat{\theta}_n(x_1, \ldots, x_n)$ - просто какое-то число из $\mathbb{R}$, т.к. мы с самого начала зафиксировали какую-то функцию $\widehat{\theta}_n$

Также $P(\xi_1 = x_1, \ldots, \xi_n = x_n) = P(\xi_1 = x_1)\times \cdots \times P(\xi_n = x_n)$ в силу независимости и $P(\xi_i = x_i) = p^{x_i}\cdot (1-p)^{1-x_i}$

$$
= \sum\limits_{x_1=0}^1 \ldots \sum\limits_{x_n=0}^1 \widehat{\theta}_n(x_1, \ldots, x_n) \cdot p^{\sum_{i=1}^n x_i} \cdot (1-p)^{n- \sum_{i=1}^n x_i}
$$

И это должно равняться $\ln p$.

Параметрическое множество $\Theta \in \mathbb{R}_{-}$ для данной задачи, так как $p=[0,1] \Rightarrow \ln p = [-\infty, 0]$.

Т.е. должно быть
$$
\sum\limits_{x_1=0}^1 \ldots \sum\limits_{x_n=0}^1 \widehat{\theta}_n(x_1, \ldots, x_n) \cdot p^{\sum_{i=1}^n x_i} \cdot (1-p)^{n- \sum_{i=1}^n x_i} = \ln p
$$

Но $\ln p$ не может равняться многочлену по всей полупрямой (отрицательной части прямой). И то же будет, если заменить $\ln p$ на $1/p$.